
import asyncio
import json
import logging
import os
import re
import aiohttp
from datetime import datetime

from src import config
from src.clients import gemini, semantic_scholar
from src.processing import document_parser
from src.storage import database

logger = logging.getLogger(__name__)

def load_papers_config():
    """papers.json ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(config.PAPERS_CONFIG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logger.error(f"`{config.PAPERS_CONFIG_FILE}` íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []

def save_summary_to_md(paper, summary, target_paper_alias, classification):
    """ìš”ì•½ ë‚´ìš©ì„ ë™ì  í´ë” êµ¬ì¡°ì— Markdown íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    is_base_summary = classification == '_base'
    
    if is_base_summary:
        output_dir = os.path.join(config.SUMMARY_DIR, target_paper_alias)
        filename = "_base_summary.md"
    else:
        output_dir = os.path.join(config.SUMMARY_DIR, target_paper_alias, classification)
        safe_title = re.sub(r'[\\/*?:":<>|]', "", paper['title'])
        filename = f"{safe_title} ({paper.get('year', 'N/A')}).md"

    os.makedirs(output_dir, exist_ok=True)
    filepath = os.path.join(output_dir, filename)

    md_content = f"# {paper['title']} ({paper.get('year', 'N/A')})\n\n"
    if paper.get('url'):
        md_content += f"**ğŸ”— ë§í¬:** [{paper['url']}]({paper['url']})\n\n"
    md_content += f"---\n\n{summary}"
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(md_content.strip())
    logger.info(f"ìš”ì•½ë³¸ ì €ì¥ ì™„ë£Œ: {filepath}")

async def process_citing_paper(session, conn, citing_paper, target_paper_details, target_alias):
    """ë‹¨ì¼ ì¸ìš© ë…¼ë¬¸ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë¶„ë¥˜, ìš”ì•½, ì €ì¥)."""
    paper_id = citing_paper.get('paperId')
    paper_title = citing_paper.get('title', 'N/A')

    if not paper_id:
        logger.warning(f"IDê°€ ì—†ëŠ” ì¸ìš© ë…¼ë¬¸ì„ ê±´ë„ˆëœë‹ˆë‹¤: {paper_title}")
        return

    try:
        # --- 1ë‹¨ê³„ ë¶„ë¥˜ ---
        api_abstract = citing_paper.get('abstract')
        first_pass_class = "uncertain"

        if api_abstract:
            # ê²½ë¡œ A: API ì´ˆë¡ì´ ìˆì„ ê²½ìš°
            logger.debug(f"'{paper_title}' - ê²½ë¡œ A: API ì´ˆë¡ìœ¼ë¡œ 1ì°¨ ë¶„ë¥˜ ìˆ˜í–‰")
            first_pass_class = await gemini.first_pass_classify_with_abstract(
                target_paper_details, api_abstract
            )
        else:
            # ê²½ë¡œ B: API ì´ˆë¡ì´ ì—†ì„ ê²½ìš° (Smarter Fallback)
            logger.debug(f"'{paper_title}' - ê²½ë¡œ B: Raw text ì¼ë¶€ë¡œ 1ì°¨ ë¶„ë¥˜ ìˆ˜í–‰")
            try:
                raw_text_snippet = await document_parser.extract_raw_text(session, citing_paper, pages=3)
                first_pass_class = await gemini.first_pass_classify_with_snippet(
                    target_paper_details, raw_text_snippet
                )
                # í›„ì† ë‹¨ê³„ë¥¼ ìœ„í•´ ì´ˆë¡ì„ ì±„ì›Œë„£ìŒ
                citing_paper['abstract'] = raw_text_snippet[:1500]
            except document_parser.PDFExtractionError as e:
                logger.error(f"'{paper_title}' ë…¼ë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨ (1ì°¨ ë¶„ë¥˜ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ): {e}")
                database.record_failure(conn, paper_id, str(e))
                return

        logger.info(f"'{paper_title}' 1ì°¨ ë¶„ë¥˜ ê²°ê³¼: {first_pass_class}")
        
        # --- 2ë‹¨ê³„ ë¶„ë¥˜ (í•„ìš” ì‹œ) ---
        classification = "other"
        structured_text = None

        if first_pass_class == "uncertain":
            logger.info(f"'{paper_title}' 2ë‹¨ê³„ ì •ë°€ ë¶„ì„ ì§„í–‰...")
            try:
                # ì „ì²´ Raw Text ì¶”ì¶œ
                full_raw_text = await document_parser.extract_raw_text(session, citing_paper)
                if not citing_paper.get('abstract'): # ìŠ¤ë‹ˆí«ì—ì„œë„ ì´ˆë¡ì„ ëª»ê°€ì ¸ì˜¨ ê²½ìš°
                    citing_paper['abstract'] = full_raw_text[:1500]

                # í…ìŠ¤íŠ¸ êµ¬ì¡°í™” (ë¹„ìš© ë°œìƒ)
                structured_text = await document_parser.structure_text(full_raw_text)
                
                # ìµœì¢… ë¶„ë¥˜
                classification = await gemini.full_text_classify(
                    target_paper_details, citing_paper, structured_text
                )
            except document_parser.PDFExtractionError as e:
                logger.error(f"'{paper_title}' ë…¼ë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨ (2ì°¨ ë¶„ë¥˜ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ): {e}")
                database.record_failure(conn, paper_id, str(e))
                return
        else: # 'same_task'
            logger.info(f"'{paper_title}' 1ì°¨ ë¶„ë¥˜ í†µê³¼, 2ë‹¨ê³„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            classification = "same_task"

        logger.info(f"'{paper_title}' ìµœì¢… ë¶„ë¥˜ ê²°ê³¼: {classification}")

        # --- ìš”ì•½ ë° ì €ì¥ ---
        summary = await gemini.summarize_with_gemini(target_paper_details, citing_paper)
        if not summary:
            logger.warning(f"'{paper_title}' ë…¼ë¬¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨. ê±´ë„ˆëœë‹ˆë‹¤.")
            database.record_failure(conn, paper_id, "ìš”ì•½ ìƒì„± ì‹¤íŒ¨")
            return
            
        save_summary_to_md(citing_paper, summary, target_alias, classification)
        database.add_paper_to_history(conn, paper_id, status='processed')

    except gemini.GeminiAPIError as e:
        logger.critical(f"'{paper_title}' ì²˜ë¦¬ ì¤‘ Gemini API ì˜¤ë¥˜: {e}")
        database.record_failure(conn, paper_id, f"Gemini API Error: {e}")
    except Exception as e:
        logger.exception(f"'{paper_title}' ë…¼ë¬¸ ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ")
        database.record_failure(conn, paper_id, f"Unexpected Error: {e}")


async def run_cycle():
    """ì—ì´ì „íŠ¸ì˜ ë©”ì¸ ì‹¤í–‰ ì‚¬ì´í´."""
    logger.info(f"--- [{datetime.now()}] ìƒˆë¡œìš´ ì‚¬ì´í´ ì‹œì‘ ---")
    
    target_papers = load_papers_config()
    if not target_papers:
        logger.warning(f"`{config.PAPERS_CONFIG_FILE}`ì— ì¶”ì í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    db_conn = database.get_db_connection(config.DB_PATH)
    
    async with aiohttp.ClientSession() as session:
        for target_paper_config in target_papers:
            target_id = target_paper_config.get("id")
            target_alias = target_paper_config.get("alias", target_id)
            target_alias = re.sub(r'[\\/*?:":<>|]', "", target_alias)
            logger.info(f"\n>> '{target_alias}' ë…¼ë¬¸ ì²˜ë¦¬ ì‹œì‘...")

            target_paper_details = await semantic_scholar.fetch_paper_details(session, target_id)
            if not target_paper_details:
                logger.error(f"'{target_alias}' ì •ë³´ ì¡°íšŒë¥¼ ì‹¤íŒ¨í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            # (ìƒëµ) ê¸°ì¤€ ë…¼ë¬¸ ìš”ì•½ ë¡œì§ì€ í•„ìš” ì‹œ ì—¬ê¸°ì— ì¶”ê°€
            
            citations = await semantic_scholar.fetch_citations(session, target_id)
            if not citations:
                logger.info(f"'{target_alias}'ì— ëŒ€í•œ ìƒˆë¡œìš´ ì¸ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                continue

            citing_paper_ids = [item['citingPaper']['paperId'] for item in citations if item.get('citingPaper', {}).get('paperId')]
            
            # DBì—ì„œ ì´ë¯¸ ì²˜ë¦¬í–ˆê±°ë‚˜ ì‹¤íŒ¨í•œ ë…¼ë¬¸ ì œì™¸
            papers_to_process_ids = database.get_papers_to_process(db_conn, citing_paper_ids)
            
            papers_to_process = [
                item['citingPaper'] for item in citations 
                if item.get('citingPaper', {}).get('paperId') in papers_to_process_ids
            ]

            if not papers_to_process:
                logger.info(f"'{target_alias}'ì˜ ëª¨ë“  ì‹ ê·œ ì¸ìš©ì€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆê±°ë‚˜ ì‹¤íŒ¨ ëª©ë¡ì— ìˆìŠµë‹ˆë‹¤.")
                continue
            
            logger.info(f"'{target_alias}'ì— ëŒ€í•´ ì²˜ë¦¬í•  ì‹ ê·œ ì¸ìš© {len(papers_to_process)}ê°œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

            # ë™ì‹œ ì²˜ë¦¬ ì‘ì—… ìƒì„±
            tasks = [
                process_citing_paper(session, db_conn, paper, target_paper_details, target_alias)
                for paper in papers_to_process[:config.MAX_CITATIONS_TO_PROCESS_PER_RUN]
            ]
            
            await asyncio.gather(*tasks)

    db_conn.close()
    logger.info("--- ì‚¬ì´í´ ì¢…ë£Œ ---")
