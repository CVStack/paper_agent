import asyncio
import json
import logging
import os
import re
import aiohttp
from datetime import datetime

from src import config
from src.clients import gemini, semantic_scholar
from src.processing import document_parser
from src.storage import database

logger = logging.getLogger(__name__)

SURVEY_KEYWORDS = [
    "survey", "review", "overview", "roadmap", 
    "state of the art", "state-of-the-art", 
    "comprehensive study", "meta-analysis"
]

def _is_survey_paper(paper):
    # 1. ë©”íƒ€ë°ì´í„° ì²´í¬
    pub_types = paper.get('publicationTypes') or []
    if 'Review' in pub_types:
        return True
        
    # 2. ì œëª© í‚¤ì›Œë“œ ì²´í¬
    title = paper.get('title', '').lower()
    if any(keyword in title for keyword in SURVEY_KEYWORDS):
        return True
        
    return False

def load_papers_config():
    """papers.json ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(config.PAPERS_CONFIG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logger.error(f"`{config.PAPERS_CONFIG_FILE}` íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []

def save_summary_to_md(paper, summary, target_paper_alias, classification):
    """ìš”ì•½ ë‚´ìš©ì„ ë™ì  í´ë” êµ¬ì¡°ì— Markdown íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    is_base_summary = classification == '_base'
    
    if is_base_summary:
        output_dir = os.path.join(config.SUMMARY_DIR, target_paper_alias)
        filename = "_base_summary.md"
    else:
        output_dir = os.path.join(config.SUMMARY_DIR, target_paper_alias, classification)
        safe_title = re.sub(r'[\\/*?:"<>|]', "", paper['title'])
        filename = f"{safe_title} ({paper.get('year', 'N/A')}).md"

    os.makedirs(output_dir, exist_ok=True)
    filepath = os.path.join(output_dir, filename)

    md_content = f"# {paper['title']} ({paper.get('year', 'N/A')})\n\n"
    if paper.get('url'):
        md_content += f"**ğŸ”— ë§í¬:** [{paper['url']}]({paper['url']})\n\n"
    md_content += f"---\n\n{summary}"
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(md_content.strip())
    logger.info(f"ìš”ì•½ë³¸ ì €ì¥ ì™„ë£Œ: {filepath}")

async def process_citing_paper(session, conn, citing_paper, target_paper_details, target_alias):
    """ë‹¨ì¼ ì¸ìš© ë…¼ë¬¸ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë¶„ë¥˜, ìš”ì•½, ì €ì¥)."""
    paper_id = citing_paper.get('paperId')
    paper_title = citing_paper.get('title', 'N/A')

    if not paper_id:
        logger.warning(f"IDê°€ ì—†ëŠ” ì¸ìš© ë…¼ë¬¸ì„ ê±´ë„ˆì›ë‹ˆë‹¤: {paper_title}")
        return

    # [NEW] ì„œë² ì´ ë…¼ë¬¸ í•„í„°ë§
    if _is_survey_paper(citing_paper):
        logger.info(f"'{paper_title}' - ì„œë² ì´ ë…¼ë¬¸ìœ¼ë¡œ ê°ì§€ë˜ì–´ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        database.record_failure(conn, paper_id, "Skipped: Survey/Review Paper") 
        return

    try:
        # --- 1ë‹¨ê³„ ë¶„ë¥˜ (Fast Filter) ---
        api_abstract = citing_paper.get('abstract')
        first_pass_class = "other"

        if api_abstract:
            # ê²½ë¡œ A: API ì´ˆë¡ì´ ìˆì„ ê²½ìš°
            logger.debug(f"'{paper_title}' - ê²½ë¡œ A: API ì´ˆë¡ìœ¼ë¡œ 1ì°¨ ë¶„ë¥˜ ìˆ˜í–‰")
            first_pass_class = await gemini.first_pass_classify_with_abstract(
                target_paper_details, api_abstract
            )
        else:
            # ê²½ë¡œ B: API ì´ˆë¡ì´ ì—†ì„ ê²½ìš° (Smarter Fallback)
            logger.debug(f"'{paper_title}' - ê²½ë¡œ B: Raw text ì¼ë¶€ë¡œ 1ì°¨ ë¶„ë¥˜ ìˆ˜í–‰")
            try:
                raw_text_snippet = await document_parser.extract_raw_text(session, citing_paper, pages=3)
                first_pass_class = await gemini.first_pass_classify_with_snippet(
                    target_paper_details, raw_text_snippet
                )
                # í›„ì† ë‹¨ê³„ë¥¼ ìœ„í•´ ì´ˆë¡ì„ ì±„ì›Œë„£ìŒ
                citing_paper['abstract'] = raw_text_snippet[:1500]
            except document_parser.PDFExtractionError as e:
                logger.error(f"'{paper_title}' ë…¼ë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨ (1ì°¨ ë¶„ë¥˜ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ): {e}")
                database.record_failure(conn, paper_id, str(e))
                return

        logger.info(f"'{paper_title}' 1ì°¨ ë¶„ë¥˜ ê²°ê³¼: {first_pass_class}")
        
        # [Strict Mode] 1ë‹¨ê³„ì—ì„œ same_taskê°€ ì•„ë‹ˆë©´ ì¦‰ì‹œ ì¢…ë£Œ
        if first_pass_class != "same_task":
            logger.info(f"'{paper_title}' - 1ë‹¨ê³„ ë¶„ë¥˜ íƒˆë½ ({first_pass_class}). ì²˜ë¦¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            # DBì— 'filtered' ë“±ìœ¼ë¡œ ê¸°ë¡í•˜ê±°ë‚˜ ê·¸ëƒ¥ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ì¢…ë£Œ
            # ì—¬ê¸°ì„œëŠ” processedê°€ ì•„ë‹ˆë¯€ë¡œ ë‹¤ìŒì— ë‹¤ì‹œ ì‹œë„ë˜ì§€ ì•Šë„ë¡ failureë¡œ ê¸°ë¡í•˜ë˜ ì‚¬ìœ ë¥¼ ëª…ì‹œ
            database.record_failure(conn, paper_id, f"Filtered: {first_pass_class}")
            return

        # --- 2ë‹¨ê³„: ë³¸ë¬¸ í™•ë³´ ë° ì •ë°€ ê²€ì¦ (Deep Verification) ---
        logger.info(f"'{paper_title}' - 1ë‹¨ê³„ í†µê³¼! ë³¸ë¬¸ í™•ë³´ ë° ì •ë°€ ê²€ì¦ ì‹œì‘.")
        
        try:
            # 1. ë³¸ë¬¸ ë‹¤ìš´ë¡œë“œ ë° êµ¬ì¡°í™” (í•„ìˆ˜)
            full_raw_text = await document_parser.extract_raw_text(session, citing_paper)
            structured_text = await document_parser.structure_text(full_raw_text)
            
            # 2. 2ë‹¨ê³„ ì •ë°€ ë¶„ë¥˜ (Double Check)
            final_classification = await gemini.full_text_classify(
                target_paper_details, citing_paper, structured_text
            )
            
            if final_classification != "same_task":
                logger.info(f"'{paper_title}' - 2ë‹¨ê³„ ì •ë°€ ë¶„ë¥˜ íƒˆë½. ì²˜ë¦¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                database.record_failure(conn, paper_id, "Filtered: 2nd pass failed")
                return

            # --- 3ë‹¨ê³„: ì‹¬ì¸µ ìš”ì•½ ë° ì €ì¥ (High Quality Summary) ---
            logger.info(f"'{paper_title}' - ìµœì¢… í•©ê²©! ì‹¬ì¸µ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            summary = await gemini.summarize_with_gemini(
                target_paper_details, citing_paper, full_text=structured_text
            )
            
            if not summary:
                logger.warning(f"'{paper_title}' ë…¼ë¬¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨. ê±´ë„ˆëœë‹ˆë‹¤.")
                database.record_failure(conn, paper_id, "ìš”ì•½ ìƒì„± ì‹¤íŒ¨")
                return
                
            save_summary_to_md(citing_paper, summary, target_alias, final_classification)
            database.add_paper_to_history(conn, paper_id, status='processed')

        except document_parser.PDFExtractionError as e:
            logger.error(f"'{paper_title}' ë³¸ë¬¸ í™•ë³´ ì‹¤íŒ¨: {e}")
            database.record_failure(conn, paper_id, f"PDF Error: {e}")
            return

    except gemini.GeminiAPIError as e:
        logger.critical(f"'{paper_title}' ì²˜ë¦¬ ì¤‘ Gemini API ì˜¤ë¥˜: {e}")
        database.record_failure(conn, paper_id, f"Gemini API Error: {e}")
    except Exception as e:
        logger.exception(f"'{paper_title}' ë…¼ë¬¸ ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ")
        database.record_failure(conn, paper_id, f"Unexpected Error: {e}")


async def run_cycle():
    """ì—ì´ì „íŠ¸ì˜ ë©”ì¸ ì‹¤í–‰ ì‚¬ì´í´."""
    logger.info(f"--- [{datetime.now()}] ìƒˆë¡œìš´ ì‚¬ì´í´ ì‹œì‘ ---")
    
    target_papers = load_papers_config()
    if not target_papers:
        logger.warning(f"`{config.PAPERS_CONFIG_FILE}`ì— ì¶”ì í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    db_conn = database.get_db_connection(config.DB_PATH)
    
    async with aiohttp.ClientSession() as session:
        for target_paper_config in target_papers:
            target_id = target_paper_config.get("id")
            target_alias = target_paper_config.get("alias", target_id)
            target_alias = re.sub(r'[\\/*?:"<>|]', "", target_alias)
            logger.info(f"\n>> '{target_alias}' ë…¼ë¬¸ ì²˜ë¦¬ ì‹œì‘...")

            target_paper_details = await semantic_scholar.fetch_paper_details(session, target_id)
            if not target_paper_details:
                logger.error(f"'{target_alias}' ì •ë³´ ì¡°íšŒë¥¼ ì‹¤íŒ¨í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            # (ìƒëµ) ê¸°ì¤€ ë…¼ë¬¸ ìš”ì•½ ë¡œì§ì€ í•„ìš” ì‹œ ì—¬ê¸°ì— ì¶”ê°€
            
            citations = await semantic_scholar.fetch_citations(session, target_id)
            if not citations:
                logger.info(f"'{target_alias}'ì— ëŒ€í•œ ìƒˆë¡œìš´ ì¸ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                continue

            citing_paper_ids = [item['citingPaper']['paperId'] for item in citations if item.get('citingPaper', {}).get('paperId')]
            
            # DBì—ì„œ ì´ë¯¸ ì²˜ë¦¬í–ˆê±°ë‚˜ ì‹¤íŒ¨í•œ ë…¼ë¬¸ ì œì™¸
            papers_to_process_ids = database.get_papers_to_process(db_conn, citing_paper_ids)
            
            papers_to_process = [
                item['citingPaper'] for item in citations 
                if item.get('citingPaper', {}).get('paperId') in papers_to_process_ids
            ]

            if not papers_to_process:
                logger.info(f"'{target_alias}'ì˜ ëª¨ë“  ì‹ ê·œ ì¸ìš©ì€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆê±°ë‚˜ ì‹¤íŒ¨ ëª©ë¡ì— ìˆìŠµë‹ˆë‹¤.")
                continue
            
            if config.MAX_CITATIONS_TO_PROCESS_PER_RUN == -1:
                # -1ì´ë©´ ëª¨ë“  ë…¼ë¬¸ì„ ì²˜ë¦¬
                limited_papers_to_process = papers_to_process
                logger.info(f"'{target_alias}'ì— ëŒ€í•´ ì œí•œ ì—†ì´ ëª¨ë“  ì‹ ê·œ ì¸ìš© {len(limited_papers_to_process)}ê°œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            else:
                # ì„¤ì •ëœ ê°’ë§Œí¼ë§Œ ì²˜ë¦¬
                limited_papers_to_process = papers_to_process[:config.MAX_CITATIONS_TO_PROCESS_PER_RUN]
                logger.info(f"'{target_alias}'ì— ëŒ€í•´ ì²˜ë¦¬í•  ì‹ ê·œ ì¸ìš© {len(limited_papers_to_process)}ê°œ (ìµœëŒ€ {config.MAX_CITATIONS_TO_PROCESS_PER_RUN}ê°œ)ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

            # ë™ì‹œ ì²˜ë¦¬ ì‘ì—… ìƒì„±
            tasks = [
                process_citing_paper(session, db_conn, paper, target_paper_details, target_alias)
                for paper in limited_papers_to_process
            ]
            
            await asyncio.gather(*tasks)

    db_conn.close()
    logger.info("--- ì‚¬ì´í´ ì¢…ë£Œ ---")