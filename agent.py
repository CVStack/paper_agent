import os
import json
import re
import sys
import time
import requests
import io
import pypdf
import google.generativeai as genai
from google.api_core import exceptions
from dotenv import load_dotenv
from datetime import datetime

# 1. ì„¤ì •
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
SEMANTIC_API_KEY = os.getenv("SEMANTIC_SCHOLAR_API_KEY")

# --- íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ìƒìˆ˜ ---
PAPERS_CONFIG_FILE = "papers.json"
HISTORY_FILE = "history.json"
SUMMARY_DIR = "summaries"
BASE_SUMMARY_PROMPT_FILE = "prompts/base_summary_prompt.md"
SUMMARY_PROMPT_FILE = "prompts/summary_prompt.md"
CLASSIFICATION_PROMPT_FILE = "prompts/classification_prompt.md"

# --- ì‹¤í–‰ ì„¤ì • ìƒìˆ˜ ---
CHECK_INTERVAL = 3600  # ì‘ì—… ë°˜ë³µ ì£¼ê¸° (ì´ˆ)
MAX_RETRIES = 5
INITIAL_DELAY = 1
CLASSIFICATION_MODEL_NAME = os.getenv("CLASSIFICATION_MODEL", "gemini-1.5-flash")
SUMMARIZATION_MODEL_NAME = os.getenv("SUMMARIZATION_MODEL", "gemini-2.5-pro")

def load_json_file(file_path, default_value):
    """JSON íŒŒì¼ì„ ë¡œë“œ, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜"""
    if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
        return default_value
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_json_file(file_path, data):
    """JSON íŒŒì¼ ì €ì¥"""
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def load_papers_config():
    """papers.json ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    return load_json_file(PAPERS_CONFIG_FILE, [])

def load_history():
    """history.json ì²˜ë¦¬ ë‚´ì—­ ë¡œë“œ"""
    return load_json_file(HISTORY_FILE, [])

def save_history(paper_id):
    """ì²˜ë¦¬ ë‚´ì—­ì— ë…¼ë¬¸ ID ì¶”ê°€"""
    history = load_history()
    if paper_id not in history:
        history.append(paper_id)
        save_json_file(HISTORY_FILE, history)

def load_prompt(prompt_file):
    """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì½ê¸°"""
    if not os.path.exists(prompt_file):
        print(f"ğŸš¨ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì—†ìŒ: {prompt_file}")
        return None
    with open(prompt_file, 'r', encoding='utf-8') as f:
        return f.read()

def _get_paper_id_for_api(paper_id):
    """API í˜¸ì¶œì„ ìœ„í•œ ë…¼ë¬¸ ID í˜•ì‹ ë§ì¶”ê¸° (e.g., ArXiv ID)"""
    if re.match(r'^\d{4}\.\d{4,5}$', paper_id):
        return f"ARXIV:{paper_id}"
    return paper_id

def fetch_paper_details(paper_id):
    """Semantic Scholarì—ì„œ íŠ¹ì • ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    api_paper_id = _get_paper_id_for_api(paper_id)
    print(f"ğŸ” ê¸°ì¤€ ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ ì¤‘: {api_paper_id}")
    url = f"https://api.semanticscholar.org/graph/v1/paper/{api_paper_id}"
    params = {"fields": "title,abstract,year,url,externalIds,openAccessPdf"}
    headers = {"x-api-key": SEMANTIC_API_KEY} if SEMANTIC_API_KEY else {}
    
    for retry_count in range(MAX_RETRIES):
        try:
            res = requests.get(url, params=params, headers=headers)
            res.raise_for_status()
            return res.json()
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                delay = INITIAL_DELAY * (2 ** retry_count)
                print(f"âš ï¸ 429 Rate Limit Hit. Retrying in {delay} seconds (Retry {retry_count + 1}/{MAX_RETRIES})...")
                time.sleep(delay)
            else:
                print(f"âŒ ê¸°ì¤€ ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜ (HTTP Error): {e}")
                return None
        except Exception as e:
            print(f"âŒ ê¸°ì¤€ ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return None
    print(f"ğŸš¨ {MAX_RETRIES}ë²ˆì˜ ì¬ì‹œë„ í›„ì—ë„ ê¸°ì¤€ ë…¼ë¬¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {api_paper_id}")
    return None

def fetch_citations(paper_id):
    """Semantic Scholarì—ì„œ ì¸ìš© ë…¼ë¬¸ ì¡°íšŒ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    api_paper_id = _get_paper_id_for_api(paper_id)
    print(f"ğŸ“„ {api_paper_id}ì˜ ì‹ ê·œ ì¸ìš© í™•ì¸ ì¤‘...")
    url = f"https://api.semanticscholar.org/graph/v1/paper/{api_paper_id}/citations"
    params = {"fields": "title,abstract,year,url,isOpenAccess,externalIds,openAccessPdf", "limit": 20}
    headers = {"x-api-key": SEMANTIC_API_KEY} if SEMANTIC_API_KEY else {}

    for retry_count in range(MAX_RETRIES):
        try:
            res = requests.get(url, params=params, headers=headers)
            res.raise_for_status()
            return res.json().get('data', [])
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                delay = INITIAL_DELAY * (2 ** retry_count)
                print(f"âš ï¸ 429 Rate Limit Hit. Retrying in {delay} seconds (Retry {retry_count + 1}/{MAX_RETRIES})...")
                time.sleep(delay)
            else:
                print(f"âŒ ì¸ìš© ë…¼ë¬¸ ì¡°íšŒ API ì˜¤ë¥˜ (HTTP Error): {e}")
                return []
        except Exception as e:
            print(f"âŒ ì¸ìš© ë…¼ë¬¸ ì¡°íšŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return []
    print(f"ğŸš¨ {MAX_RETRIES}ë²ˆì˜ ì¬ì‹œë„ í›„ì—ë„ ì¸ìš© ë…¼ë¬¸ ì¡°íšŒ ì‹¤íŒ¨: {api_paper_id}")
    return []

def organize_text_with_gemini(text):
    """
    Gemini 1.5 Flashë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not text:
        return {}

    print("   ğŸ¤– Gemini Flashë¡œ í…ìŠ¤íŠ¸ êµ¬ì¡°í™” ë° ì •ì œ ì¤‘ (JSON ì¶œë ¥)...")
    
    input_text = text[:30000]
    
    prompt = f"""
    ë‹¹ì‹ ì€ ë…¼ë¬¸ ë¶„ì„ AIì…ë‹ˆë‹¤. ì•„ë˜ ë…¼ë¬¸ í…ìŠ¤íŠ¸(Raw Text)ì—ì„œ í•µì‹¬ ì„¹ì…˜ì„ ì¶”ì¶œí•˜ì—¬ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
    
    [JSON ìŠ¤í‚¤ë§ˆ]
    {{
        "abstract": "ì´ˆë¡ ë‚´ìš© (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)",
        "introduction": "ì„œë¡  ë° ë¬¸ì œ ì •ì˜ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)",
        "method": "ì œì•ˆ ë°©ë²•ë¡  ë° ì•„í‚¤í…ì²˜ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)",
        "conclusion": "ê²°ë¡  ë° ìš”ì•½ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)",
        "experiments": "ì‹¤í—˜ ê²°ê³¼ ë° ë¹„êµ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)"
    }}
    
    [ì£¼ì˜ì‚¬í•­]
    1. References, Appendix, AcknowledgmentsëŠ” ì œì™¸í•˜ì„¸ìš”.
    2. ë‚´ìš©ì€ ìš”ì•½í•˜ì§€ ë§ê³  ì›ë¬¸ ë¬¸ì¥ë“¤ì„ ìµœëŒ€í•œ ìœ ì§€í•˜ì—¬ ë°œì·Œí•˜ì„¸ìš”.
    3. ì–¸ì–´ëŠ” ì›ë¬¸(ì˜ì–´) ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.
    4. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

    [ì›ë³¸ í…ìŠ¤íŠ¸]
    {input_text}
    """

    try:
        model = genai.GenerativeModel(
            CLASSIFICATION_MODEL_NAME,
            generation_config={"response_mime_type": "application/json"}
        )
        response = model.generate_content(prompt)
        return json.loads(response.text)
    except Exception as e:
        print(f"   âš ï¸ í…ìŠ¤íŠ¸ êµ¬ì¡°í™”(JSON) ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"full_text": text[:20000]} # ì‹¤íŒ¨ ì‹œ ì›ë³¸ì„ í†µì§¸ë¡œ ë°˜í™˜

def fetch_full_text(paper_details):
    """
    ë…¼ë¬¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ìš°ì„ ìˆœìœ„:
    1. ArXiv IDê°€ ìˆëŠ” ê²½ìš° ArXiv PDF ì„œë²„ ì§ì ‘ ì´ìš©
    2. Semantic Scholarê°€ ì œê³µí•˜ëŠ” openAccessPdf ë§í¬ ì´ìš©
    3. ì¼ë°˜ urlì´ pdfë¡œ ëë‚˜ëŠ” ê²½ìš° ì´ìš©
    """
    pdf_url = None
    
    # 1. ArXiv ID í™•ì¸ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
    external_ids = paper_details.get('externalIds') or {}
    arxiv_id = external_ids.get('ArXiv')
    if arxiv_id:
        pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
        print(f"   ğŸ¯ ArXiv ID ë°œê²¬: {arxiv_id} -> {pdf_url}")
    
    # 2. Semantic Scholar ì œê³µ Open Access PDF í™•ì¸
    if not pdf_url:
        open_access_pdf = paper_details.get('openAccessPdf')
        if open_access_pdf and open_access_pdf.get('url'):
            pdf_url = open_access_pdf.get('url')
            print(f"   ğŸ¯ OpenAccess PDF ë§í¬ ë°œê²¬: {pdf_url}")

    # 3. ì¼ë°˜ URL í™•ì¸ (Fallback)
    if not pdf_url:
        url = paper_details.get('url', '')
        if 'arxiv.org/abs/' in url:
            pdf_url = url.replace('/abs/', '/pdf/') + '.pdf'
        elif url.endswith('.pdf'):
            pdf_url = url
    
    if not pdf_url:
        return ""

    try:
        print(f"   â¬‡ï¸ PDF ë‹¤ìš´ë¡œë“œ ì‹œë„: {pdf_url}")
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
        response = requests.get(pdf_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        pdf_file = io.BytesIO(response.content)
        reader = pypdf.PdfReader(pdf_file)
        text = ""
        for page in reader.pages:
            text += page.extract_text() or ""
        
        print(f"   âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ ({len(text)}ì). AI êµ¬ì¡°í™” ì§„í–‰...")
        
        # Gemini Flashë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ êµ¬ì¡°í™” (JSON ë°˜í™˜)
        structured_data = organize_text_with_gemini(text)
        
        # JSON ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if "full_text" in structured_data:
             print("   âš ï¸ êµ¬ì¡°í™” ì‹¤íŒ¨ë¡œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜.")
             return structured_data["full_text"]

        # JSON ë°ì´í„°ë¥¼ ë³´ê¸° ì¢‹ì€ Markdownìœ¼ë¡œ ë³€í™˜
        md_output = ""
        if structured_data.get("abstract"):
            md_output += f"## Abstract\n{structured_data['abstract']}\n\n"
        if structured_data.get("introduction"):
            md_output += f"## Introduction\n{structured_data['introduction']}\n\n"
        if structured_data.get("method"):
            md_output += f"## Method\n{structured_data['method']}\n\n"
        if structured_data.get("conclusion"):
            md_output += f"## Conclusion\n{structured_data['conclusion']}\n\n"
        if structured_data.get("experiments"):
            md_output += f"## Experiments\n{structured_data['experiments']}\n\n"
            
        print(f"   âœ¨ í…ìŠ¤íŠ¸ êµ¬ì¡°í™” ì™„ë£Œ (Markdown ë³€í™˜ë¨, {len(md_output)}ì).")
        return md_output

    except Exception as e:
        print(f"   âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return ""

def classify_paper(citing_paper, target_paper_details):
    """Geminië¥¼ ì‚¬ìš©í•´ ë…¼ë¬¸ì„ 'same_task' ë˜ëŠ” 'other'ë¡œ ë¶„ë¥˜"""
    print(f"ğŸ§ '{citing_paper['title']}' ë…¼ë¬¸ ë¶„ë¥˜ ì¤‘...")
    classification_prompt = load_prompt(CLASSIFICATION_PROMPT_FILE)
    if not classification_prompt:
        return 'other'

    full_text = fetch_full_text(citing_paper)

    prompt = classification_prompt.replace('{{target_title}}', target_paper_details.get('title', '')).replace('{{target_abstract}}', target_paper_details.get('abstract', '')).replace('{{title}}', citing_paper.get('title', '')).replace('{{abstract}}', citing_paper.get('abstract', 'ì´ˆë¡ ì •ë³´ ì—†ìŒ')).replace('{{full_text}}', full_text)

    try:
        model = genai.GenerativeModel(CLASSIFICATION_MODEL_NAME)
        response = model.generate_content(prompt)
        result_text = response.text.strip().lower()
        
        if "yes" in result_text:
            print("â¡ï¸ ë¶„ë¥˜ ê²°ê³¼: same_task")
            return "same_task"
        else:
            print("â¡ï¸ ë¶„ë¥˜ ê²°ê³¼: other")
            return "other"
    except exceptions.ResourceExhausted as e:
        print("\nğŸš¨ [ë¹„ìš© ë°©ì§€] Gemini API í• ë‹¹ëŸ‰(Quota)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print(f"   ì˜¤ë¥˜ ìƒì„¸: {e}")
        sys.exit()
    except Exception as e:
        print(f"âŒ ë¶„ë¥˜ ì¤‘ Gemini API ì˜¤ë¥˜: {e}")
        return "other"

def summarize_with_gemini(paper_title, formatted_prompt):
    """Geminiì—ê²Œ ìš”ì•½ ìš”ì²­"""
    print(f"ğŸ¤– '{paper_title}' ìš”ì•½ ì¤‘...")
    
    try:
        model = genai.GenerativeModel(SUMMARIZATION_MODEL_NAME)
        response = model.generate_content(formatted_prompt)
        return response.text
    except exceptions.ResourceExhausted as e:
        print("\nğŸš¨ [ë¹„ìš© ë°©ì§€] Gemini API í• ë‹¹ëŸ‰(Quota)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print(f"   ì˜¤ë¥˜ ìƒì„¸: {e}")
        sys.exit()
    except Exception as e:
        print(f"âŒ ìš”ì•½ ì¤‘ Gemini API ì˜¤ë¥˜: {e}")
        return ""

def save_summary_to_md(paper, summary, target_paper_alias, classification):
    """ìš”ì•½ ë‚´ìš©ì„ ë™ì  í´ë” êµ¬ì¡°ì— Markdown íŒŒì¼ë¡œ ì €ì¥"""
    is_base_summary = classification == '_base'
    
    if is_base_summary:
        output_dir = os.path.join(SUMMARY_DIR, target_paper_alias)
        filename = "_base_summary.md"
    else:
        output_dir = os.path.join(SUMMARY_DIR, target_paper_alias, classification)
        safe_title = re.sub(r'[\\/*?:"<>|]', "", paper['title'])
        filename = f"{safe_title} ({paper.get('year', 'N/A')}).md"

    os.makedirs(output_dir, exist_ok=True)
    filepath = os.path.join(output_dir, filename)

    md_content = f"# {paper['title']} ({paper.get('year', 'N/A')})\n\n"
    if paper.get('url'):
        md_content += f"**ğŸ”— ë§í¬:** [{paper['url']}]({paper['url']})\n\n"
    md_content += f"---\n\n{summary}"
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(md_content.strip())
    print(f"âœ… ìš”ì•½ë³¸ ì €ì¥ ì™„ë£Œ: {filepath}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    while True:
        print(f"\n--- [{datetime.now()}] ìƒˆë¡œìš´ ì‚¬ì´í´ ì‹œì‘ ---")
        
        target_papers = load_papers_config()
        if not target_papers:
            print("ğŸš¨ `papers.json`ì— ì¶”ì í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            time.sleep(CHECK_INTERVAL)
            continue

        base_summary_prompt_template = load_prompt(BASE_SUMMARY_PROMPT_FILE)
        summary_prompt_template = load_prompt(SUMMARY_PROMPT_FILE)

        if not base_summary_prompt_template or not summary_prompt_template:
            print("ğŸš¨ í•„ìˆ˜ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        history = load_history()

        for target_paper in target_papers:
            target_id = target_paper.get("id")
            target_alias = target_paper.get("alias", target_id)
            target_alias = re.sub(r'[\\/*?:"<>|]', "", target_alias)
            print(f"\n>> '{target_alias}' ë…¼ë¬¸ ì²˜ë¦¬ ì‹œì‘...")

            target_paper_details = fetch_paper_details(target_id)
            if not target_paper_details:
                print(f"   '{target_alias}' ì •ë³´ ì¡°íšŒë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            base_summary_path = os.path.join(SUMMARY_DIR, target_alias, '_base_summary.md')
            if not os.path.exists(base_summary_path):
                print(f"   '{target_alias}'ì˜ ê¸°ì¤€ ë…¼ë¬¸ ìš”ì•½ë³¸ì´ ì—†ìŠµë‹ˆë‹¤. ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                
                full_text = fetch_full_text(target_paper_details)
                if not full_text:
                    print("   âš ï¸ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨. ì´ˆë¡(Abstract)ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                    full_text = target_paper_details.get('abstract', 'ë‚´ìš© ì—†ìŒ')

                formatted_prompt = base_summary_prompt_template.replace('{{title}}', target_paper_details.get('title','')).replace('{{full_text}}', full_text)
                base_summary = summarize_with_gemini(target_paper_details.get('title', ''), formatted_prompt)
                
                if base_summary:
                    save_summary_to_md(target_paper_details, base_summary, target_alias, '_base')
            
            time.sleep(1) 
            citations = fetch_citations(target_id)
            new_papers_found_for_target = False
            processed_count = 0
            MAX_TO_PROCESS = 3

            for item in citations:
                citing_paper = item.get('citingPaper', {})
                if not citing_paper.get('paperId'):
                    continue
                
                if citing_paper['paperId'] in history:
                    continue
                
                new_papers_found_for_target = True
                
                # API ì´ˆë¡ì´ ì—†ìœ¼ë©´ ì›ë¬¸ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ì‹œë„
                api_abstract = citing_paper.get('abstract')
                if not api_abstract or api_abstract == "ì´ˆë¡ ì •ë³´ ì—†ìŒ": # API ì´ˆë¡ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš°
                    print(f"   API ì´ˆë¡ ì—†ìŒ. '{citing_paper.get('title')}' ë…¼ë¬¸ì˜ ì›ë¬¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„...")
                    full_text_from_pdf = fetch_full_text(citing_paper)
                    if full_text_from_pdf:
                        # ì›ë¬¸ì—ì„œ ì´ˆë¡ ëŒ€ìš©ìœ¼ë¡œ ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ì„¤ì •
                        citing_paper['abstract'] = full_text_from_pdf[:1500] if len(full_text_from_pdf) > 1500 else full_text_from_pdf
                        print(f"   âœ… ì›ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ì´ˆë¡ ëŒ€ìš©ìœ¼ë¡œ {len(citing_paper['abstract'])}ì ì¶”ì¶œ ì„±ê³µ.")
                    else:
                        citing_paper['abstract'] = "ì´ˆë¡ ì •ë³´ ì—†ìŒ"
                
                if citing_paper['abstract'] == "ì´ˆë¡ ì •ë³´ ì—†ìŒ":
                    print(f"   '{citing_paper.get('title')}' ë…¼ë¬¸ì€ ìµœì¢…ì ìœ¼ë¡œ ì´ˆë¡ ì •ë³´ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                classification = classify_paper(citing_paper, target_paper_details)
                
                formatted_prompt = summary_prompt_template.replace('{{target_title}}', target_paper_details.get('title', '')).replace('{{target_abstract}}', target_paper_details.get('abstract', '')).replace('{{title}}', citing_paper.get('title', '')).replace('{{abstract}}', citing_paper.get('abstract', 'ì´ˆë¡ ì •ë³´ ì—†ìŒ'))
                summary = summarize_with_gemini(citing_paper.get('title',''), formatted_prompt)
                
                if summary:
                    save_summary_to_md(citing_paper, summary, target_alias, classification)
                    save_history(citing_paper['paperId'])
                    processed_count += 1
                
                time.sleep(1)

                if processed_count >= MAX_TO_PROCESS:
                    print(f"   í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ìµœëŒ€ {MAX_TO_PROCESS}ê°œì˜ ë…¼ë¬¸ë§Œ ì²˜ë¦¬í•˜ê³  ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
            
            if not new_papers_found_for_target:
                print(f"âœ… '{target_alias}'ì— ëŒ€í•œ ìƒˆë¡œìš´ ì¸ìš© ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")

        print(f"\n--- ì‚¬ì´í´ ì¢…ë£Œ. ë‹¤ìŒ í™•ì¸ê¹Œì§€ {CHECK_INTERVAL}ì´ˆ ëŒ€ê¸°... ---")
        time.sleep(CHECK_INTERVAL)

if __name__ == "__main__":
    main()